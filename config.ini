[Setup]
poly_coeffs_range=3 ; redundant
ulcd_range=3 ; redundant
const=e ; the target constant
ab_polys_type=normal ; normal is a_n = A0+A1*n+A2*n^2+A3*n^3+...Ak*n^k    indexed is A0*n^k+A1*(n-1)^k+A2*(n-2)^k+A3*(n-3)^k+...+Ak*(n-k)^k
a_poly_size=6 ; redundant
b_poly_size=6 ; redundant
a_interlace=1 ; redundant
b_interlace=1 ; redundant
print_surprising_nonexp_contfracs=False ; printing slow convergence cases that we thought should be fast (can be identified by the discriminant)
gen_hashtable_only=true  ; just for hashtable - not looking for clicks
;a_coeffs_range=[[[0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4]]]
;b_coeffs_range=[[[0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4]]]
a_coeffs_range=[[[0, 4], [0, 4], [0, 4], [0, 4], [0, 4]]] ; for example: [[[],[]],[[],[],[]],[[],[],[],[]],[[],[]]] 4-interlace with degrees of 2,3,4,2 . [a b] means running on coefficients between a to b-1
b_coeffs_range=[[[0, 4], [0, 4], [0, 4], [0, 4], [0, 4]]]

;lhs_params=[3, 3, 3, 3]
hashtable_num_of_iterations=300 ; constant number of iterations. probably enough for the exponential converging cases. problematic for when the discriminant is <=0 (that may create false entries in the hashtables that will be filtered after clicks)
hashtable_precision=15 ; number of digits saved in the hashtable
;lhs_type=ulcd
lhs_type=rationalfunc ; the ratio of polynoms on the left hand side
lhs_params=[[[0, 7], [0, 7], [0, 7]], [[0, 7], [0, 7], [0, 7]]] ; same parsing as the ab polynomials although here there is no need for interlacing
;lhs_params=[[[0, 2], [0, 2], [0, 2]], [[0, 2], [0, 2], [0, 2]]]
postproc_funcs_filter=["safe_inverse", "lambda x: x", "lambda x: x**2", "lambda x: safe_inverse(x**2)", "safe_sqrt", "lambda x: safe_inverse(safe_sqrt(x))"] ; the subgroup of postprocessing functions
i=3 ; for some constants we have an index telling which one (constants with no index, like e and pi, ignore this)

; options for hastable_file_opeartion: use, generate, expand
; generate will create a new file even if using an existing name (v2, v3,...)
; expand will first load the most updated file (v_latest) and then add the new entries and save to v_latest+1
; use will load the v_latest and use it (no generation of hash table)
hashtable_file_operation=generate
hashtable_file = hashtable_e_to_join.pkl ; file name without the v_number part
;hashtable_file = hash_table.pkl